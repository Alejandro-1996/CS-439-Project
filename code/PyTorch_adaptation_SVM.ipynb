{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quantitative-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunch of useful libraries + wngrad\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import scipy\n",
    "import scipy.sparse as sps\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "import random\n",
    "import helpers\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-culture",
   "metadata": {},
   "source": [
    "# SVM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sporting-riding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2477]) torch.Size([2477, 300])\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = '../labs/ex09/template/data/w1a'\n",
    "\n",
    "A, y = load_svmlight_file(DATA_TRAIN_PATH)\n",
    "A = A.toarray()\n",
    "n_instances, n_features = A.shape\n",
    "A = torch.tensor(A, dtype=torch.float, requires_grad = True)\n",
    "y = torch.tensor(y, dtype=torch.float, requires_grad = True)\n",
    "print(y.shape, A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-longer",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nearby-tooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WNGrad(torch.optim.Optimizer):\n",
    "  \n",
    "    def __init__(self, params, lr=1, lambda_=None):\n",
    "\n",
    "        defaults = dict(lr=lr, lambda_=lambda_)\n",
    "        super(WNGrad, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(WNGrad, self).__setstate__(state)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:# This enables us to re-evaluate the gradient.\n",
    "            with torch.enable_grad():\n",
    "                loss = closure() \n",
    "\n",
    "        withoutLambda = True\n",
    "        \n",
    "        # Iterate groups of parameters (aka layers in NN)\n",
    "        for group in self.param_groups:\n",
    "            # Iterate actual parameters in the layers \n",
    "            for param in group['params']:\n",
    "                print(param.grad)\n",
    "                if param.grad is not None:\n",
    "                    state = self.state[param]\n",
    "                    lambda_ = group['lambda_']\n",
    "                    # Calculate state[b_j] for a first time\n",
    "                    if len(state) == 0:\n",
    "                        b_candidates = []\n",
    "                        # Iterate a few points  \n",
    "                        for i in np.arange(-3, 3, 0.2):\n",
    "                            # Change parameters differently\n",
    "                            sigma = torch.Tensor([10]).pow(-i)\n",
    "                            params_temp = param + torch.empty_like(param, requires_grad = True).normal_(0, 10**i)\n",
    "                            params_temp.requires_grad = True\n",
    "#                             params_temp.grad = torch.autograd.Variable(torch.zeros(1,), requires_grad=True)\n",
    "                            print(params_temp.requires_grad)\n",
    "                            loss = closure(alternative_w = params_temp)\n",
    "#                             loss.requires_grad = True\n",
    "                            loss.backward()\n",
    "                            for group in self.param_groups:\n",
    "                                for param in group['params']:\n",
    "                                    if param.grad is not None:\n",
    "                                        print('Hola')\n",
    "                                        print(params_temp.grad)\n",
    "                                        b_candidates.append((param.grad-params_temp.grad).norm(2)/(param-params_temp).norm(2))\n",
    "                        print(b_candidates)\n",
    "                        state['b_j'] = max(b_candidates)\n",
    "                        print(f'initial b: ', state['b_j'])\n",
    "\n",
    "                    lr = 1/state['b_j']\n",
    "                    if lambda_ is None:\n",
    "                        #print(\"Before\", param.grad)\n",
    "                        param.sub_(param.grad, alpha=lr) #add_ is inplace.\n",
    "                        #print(\"After sub\", param, param.grad)\n",
    "                    else:\n",
    "                        # Scaling used (opposite order)\n",
    "                        withoutLambda = False\n",
    "                        grad_2_norm = param.grad.pow(2).sum().sqrt()\n",
    "                        state['b_j'] = lr + (lambda_**2)*grad_2_norm*lr\n",
    "                        param.sub_(param.grad, alpha=lambda_*lr)\n",
    "        \n",
    "        # Re evaluate the gradients so that we can update b_j based on x_t.\n",
    "        if closure is not None and withoutLambda: \n",
    "            with torch.enable_grad():\n",
    "                _ = closure()\n",
    "                # Update b_j for each param using gradient eval at x_t\n",
    "                for group in self.param_groups:\n",
    "                    for param in group['params']:\n",
    "                        if param.grad is not None:\n",
    "                            state = self.state[param]\n",
    "                            prev_bj = state['b_j']\n",
    "                            grad_2_norm = param.grad.pow(2).sum()\n",
    "                            #print(\"After\", param.grad)\n",
    "                            update = grad_2_norm/prev_bj\n",
    "                            #print(\"ratio\", ratio)\n",
    "                            state['b_j'] = prev_bj + update\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "quiet-bishop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v.requires_grad True\n",
      "Torch\n",
      "Calling Optimizer Step\n",
      "v.requires_grad True\n",
      "Torch\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "True\n",
      "Calculating with alternative w\n",
      "A grad: True output grad: False\n",
      "v.requires_grad False\n",
      "Torch\n",
      "Inside closure: y:True, A:True,                       alt_w:True, loss:False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a473475548b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_SVM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_of_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-a473475548b9>\u001b[0m in \u001b[0;36mtrain_SVM\u001b[1;34m(A, b, w, lambda_, nb_of_iter)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calling Optimizer Step'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dolly\\documents\\epfl\\ma-3\\deeplearning\\env_deepl\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dolly\\documents\\epfl\\ma-3\\deeplearning\\env_deepl\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-c8bd5f908d52>\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     44\u001b[0m                             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malternative_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m#                             loss.requires_grad = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                                 \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dolly\\documents\\epfl\\ma-3\\deeplearning\\env_deepl\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dolly\\documents\\epfl\\ma-3\\deeplearning\\env_deepl\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "def calculate_primal_objective(y, A, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\n",
    "    A: the full dataset matrix, shape = (num_examples, num_features)\n",
    "    y: the corresponding +1 or -1 labels, shape = (num_examples)\n",
    "    w: shape = (num_features)\n",
    "    \n",
    "    Note - Function designed for PyTorch\n",
    "    \"\"\"\n",
    "    if type(A) == torch.Tensor:\n",
    "        v = torch.maximum(1 - y * (A @ w), torch.zeros_like(y, requires_grad = True))\n",
    "        print('v.requires_grad', v.requires_grad)\n",
    "        loss = torch.sum(v) + lambda_ / 2 * torch.sum(w ** 2)\n",
    "        \n",
    "#         print(w.requires_grad)\n",
    "#         print(loss.requires_grad)\n",
    "#         loss.requires_grad = True \n",
    "        print('Torch')\n",
    "        return loss\n",
    "    else:\n",
    "        v = np.maximum(1 - y * (A @ w), 0)\n",
    "        return np.sum(v) + lambda_ / 2 * np.sum(w ** 2)\n",
    "\n",
    "w = torch.zeros(n_features, requires_grad=True, dtype=torch.float)\n",
    "optimizer = WNGrad([w], lr=None, lambda_=0.01)\n",
    "\n",
    "def train_SVM(A, b, w, lambda_, nb_of_iter = 5000):\n",
    "    history = defaultdict(list)\n",
    "    for i in range(nb_of_iter):\n",
    "        # 'forward pass'\n",
    "        output = helpers.prediction(A, w)\n",
    "        # Calculate loss\n",
    "        loss = calculate_primal_objective(y, A, w, lambda_)\n",
    "        loss.backward()\n",
    "        if i == nb_of_iter/10:\n",
    "            print(i)\n",
    "        def closure(alternative_w = None):\n",
    "            optimizer.zero_grad()\n",
    "            if alternative_w != None:\n",
    "                print('Calculating with alternative w')\n",
    "                output = (A @ alternative_w > 0) * 2 - 1\n",
    "                print('A grad:', A.requires_grad, 'output grad:', output.requires_grad)\n",
    "                loss = calculate_primal_objective(y, A, alternative_w, lambda_)\n",
    "                print(f'Inside closure: y:{y.requires_grad}, A:{A.requires_grad}, \\\n",
    "                      alt_w:{alternative_w.requires_grad}, loss:{loss.requires_grad}')\n",
    "            else:\n",
    "                output = (A @ w > 0) * 2 - 1\n",
    "                loss = calculate_primal_objective(y, A, w, lambda_)\n",
    "            return loss\n",
    "        optimizer.zero_grad()\n",
    "        print('Calling Optimizer Step')\n",
    "        optimizer.step(closure)\n",
    "        history['loss'].append(loss)\n",
    "        \n",
    "    return history, loss\n",
    "\n",
    "history, loss = train_SVM(A, y, w, lambda_ = 0.01, nb_of_iter = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "going-terrorism",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stochastic_gradient_descent_svm_demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-793195067353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory_sgd_4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwt_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstochastic_gradient_descent_svm_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mhistory_sgd_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwt_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstochastic_gradient_descent_svm_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhistory_sgd_35\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwt_sgd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstochastic_gradient_descent_svm_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5e-4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'stochastic_gradient_descent_svm_demo' is not defined"
     ]
    }
   ],
   "source": [
    "history_sgd_4, wt_sgd = stochastic_gradient_descent_svm_demo(A, y, 1e-4, batch_size=500, trace=True)\n",
    "history_sgd_3, wt_sgd = stochastic_gradient_descent_svm_demo(A, y, 1e-3, batch_size=500, trace=True)\n",
    "history_sgd_35, wt_sgd = stochastic_gradient_descent_svm_demo(A, y, 5e-4, batch_size=500, trace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "directed-piano",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2568cd4bac8>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogy(range(50000), history[\"loss\"], label=\"SGD, gamma = 10^{-4}\")\n",
    "# plt.semilogy(history_wngrad[\"iter\"], history_wngrad[\"objective_function\"], label=\"WNGrad\")\n",
    "# plt.semilogy(history_sgd_3[\"iter\"], history_sgd_3[\"objective_function\"], label=\"SGD, gamma = 10^{-3}\")\n",
    "# plt.semilogy(history_sgd_35[\"iter\"], history_sgd_35[\"objective_function\"], label=\"SGD, gamma = 5*10^{-4}\")\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surprising-slovak",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict, {})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SVMWNGrad():\n",
    "\n",
    "    def __init__(self, n_iter=100, lr=None, lambda_=None):\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "    def fit(self, A, y):\n",
    "\n",
    "        n_instances, n_features = X.shape\n",
    "        \n",
    "        # we need to \"wrap\" the NumPy arrays X and Y as PyTorch tensors\n",
    "        Xt = torch.tensor(A, dtype=torch.float)\n",
    "        yt = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "        # initialize the weight vector to all zeros\n",
    "        self.w = torch.zeros(n_features, requires_grad=True, dtype=torch.float)\n",
    "\n",
    "        self.history = []\n",
    "        \n",
    "        # we select an optimizer, in this case (minibatch) SGD.\n",
    "        # it needs to be told what parameters to optimize, and what learning rate (lr) to use\n",
    "        optimizer = WNGrad([self.w], lr=self.lr, lambda_=self.lambda_)\n",
    "        \n",
    "        svm_loss = helpers.calculate_primal_objective\n",
    "        # as an alternative to SGD, we could have used adaptive gradient-based optimization\n",
    "        # algorithms such as Adam. I don't think they give an improvement in this case though,\n",
    "        # since the objective function is so simple.\n",
    "        #   optimizer = torch.optim.Adam([self.w], lr=self.eta)\n",
    "        for i in range(self.n_iter):  \n",
    "            def closure(alternative_w = None):\n",
    "                optimizer.zero_grad()\n",
    "                output = (Xt @ self.w > 0) * 2 - 1\n",
    "                if alternative_w != None:\n",
    "                    loss = svm_loss(yt, At, alternative_w, lambda_)\n",
    "                else:\n",
    "                    loss = svm_loss(yt, At, alternative_w, lambda_)\n",
    "                loss.backward()\n",
    "                return loss\n",
    "\n",
    "            loss = optimizer.step(closure)\n",
    "            self.history.append(loss)\n",
    "                                      \n",
    "        print('WNGrad final loss: {:.4f}'.format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-burton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
